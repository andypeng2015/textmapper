// generated by Textmapper; DO NOT EDIT

import {TokenType as token} from './token';
import * as lt from './lexer_tables';


const bomSeq = "\xef\xbb\xbf";

function decodeRune(str: string, offset: number): { rune: number; size: number } {
  if (offset >= str.length) {
    return { rune: -1, size: 0 };
  }

  const decoder = new TextDecoder('utf-8');
  const remainingString = str.substring(offset); // Create a new string starting from the offset
  const byteArray = new TextEncoder().encode(remainingString); // Encode the string to a Uint8Array
  const decoded = decoder.decode(byteArray);

  if (decoded.length === 0) {
    return { rune: -1, size: 0 };
  }

  const firstChar = decoded.codePointAt(0)!;

  // Determine the size in bytes of the first character
  let size = 1;
  if (firstChar >= 0x80) {
    if (firstChar >= 0x800) {
      if (firstChar >= 0x10000) {
        size = 4
      } else {
        size = 3
      }
    } else {
      size = 2
    }
  }

  return { rune: firstChar, size: size };
}
// Lexer uses a generated DFA to scan through a utf-8 encoded input string. If
// the string starts with a BOM character, it gets skipped.
export class Lexer {
  _source: string;
  _ch: number;           // current character, -1 means EOI
  _offset: number;       // character offset
  _tokenOffset: number;  // last token byte offset
  _line: number;         // current line number (1-based)
  _tokenLine: number;    // last token line
  _scanOffset: number;   // scanning offset
  _value: any;


  // Initialize the lexer
  constructor(source: string) {
    this._source = source;
    this._ch = 0;
    this._offset = 0;
    this._tokenOffset = 0;
    this._line = 1;
    this._tokenLine = 1;
    this._scanOffset = 0;

    if (source.startsWith(bomSeq)) {
      this._offset += bomSeq.length;
    }
    this.rewind(this._offset)
  }

  // rewind can be used in lexer actions to accept a portion of a scanned token, or to include
  // more text into it.
  private rewind(offset: number) {
    if (offset < this._offset) {
      this._line -= (this._source.substring(offset, this._offset).match(/\n/g) || []).length;
    } else {
      if (offset > this._source.length) {
        offset = this._source.length;
      }
      this._line += (this._source.substring(this._offset, offset).match(/\n/g) || []).length;
    }

    // Scan the next character.
    this._scanOffset = offset;
    this._offset = offset;
    if (this._offset < this._source.length) {
    let r = this._source.charCodeAt(this._offset);
    let w = 1;
    if (r >= 0x80) {
      // not ASCII
      const result = decodeRune(this._source, this._offset);
      r = result.rune;
      w = result.size;
    }
    this._scanOffset += w;
    this._ch = r;
    } else {
      this._ch = -1; // EOI
    }
  }

  // next finds and returns the next token in l.source. The source end is
  // indicated by token.EOI.
  //
  // The token text can be retrieved later by calling the text() method.
  next(): token {
    restart: while(true) {
      this._tokenLine = this._line;
      this._tokenOffset = this._offset;

      let state = 0;
      let hash = 0;
      let backupToken = token.UNAVAILABLE;
      let backupOffset = 0;
      let backupHash = hash;
      for (; state >= 0; ) {
        let ch = 0;
        if (this._ch >= 0 && this._ch < lt.tmRuneClassLen) {
          ch = lt.tmRuneClass[this._ch];
        } else if (this._ch < 0) {
          state = lt.tmLexerAction[state * lt.tmNumClasses];
          if (state > lt.tmFirstRule && state < 0) {
            state = (-1 - state) * 2;
            backupToken = lt.tmBacktracking[state];
            backupOffset = this._offset;
            backupHash = hash;
            state = lt.tmBacktracking[state + 1];
          }
          continue;
        } else {
          ch = 1;
        }
        state = lt.tmLexerAction[state * lt.tmNumClasses + ch];
        if (state > lt.tmFirstRule) {
          if (state < 0) {
            state = (-1 - state) * 2;
            backupToken = lt.tmBacktracking[state];
            backupOffset = this._offset;
            backupHash = hash;
            state = lt.tmBacktracking[state + 1];
          }
          hash = hash * 31 + this._ch;
          if (this._ch === '\n'.charCodeAt(0)) {
            this._line++;
          }
          // Scan the next character.
          // Note: the following code is inlined to avoid performance implications.
          this._offset = this._scanOffset;
          if (this._offset < this._source.length) {
            let r = this._source.charCodeAt(this._offset);
            let w = 1;
            if (r >= 0x80) {
              // not ASCII
              const result = decodeRune(this._source, this._offset);
              r = result.rune;
              w = result.size;
            }
            this._scanOffset += w;
            this._ch = r;
          } else {
            this._ch = -1; // EOI
          }
        }
      }
      let tok : token = lt.tmFirstRule - state;
      recovered: while (true) {
        switch (tok) {
          case token.ID:
          switch (hash & 7) {
            case 3:
              if (hash === 0x5cb1923 && "false" === this._source.substring(this._tokenOffset, this._offset)) {
                tok = token.FALSE;
              break;
              }
            break;
            case 6:
              if (hash === 0x36758e && "true" === this._source.substring(this._tokenOffset, this._offset)) {
                tok = token.TRUE;
              break;
              }
            break;
            case 7:
              if (hash === 0x33c587 && "null" === this._source.substring(this._tokenOffset, this._offset)) {
                tok = token.NULL;
              break;
              }
            break;
          }
        }
        switch (tok) {
          case token.INVALID_TOKEN:
    // handleInvalidToken
    if (backupToken >= 0) {
      tok = backupToken;
      hash = backupHash;
      this.rewind(backupOffset);
    } else if (this._offset === this._tokenOffset) {
      if (this._ch === -1) {
        tok = token.EOI;
      }
      this.rewind(this._scanOffset);
    }
    if (tok !== token.INVALID_TOKEN) {
      continue recovered;
    }
    // End handleInvalidToken
          break;

          case 8:
            continue restart;
        }
        return tok;
      }
    }
  }

  // pos returns the start and end positions of the last token returned by Next().
  pos(): {start: number; end: number} {
    return { start: this._tokenOffset, end: this._offset};
  }

    // line returns the line number of the last token returned by next() (1-based).
  line(): number {
    return this._tokenLine;
  }

  // text returns the substring of the input corresponding to the last token.
  text(): string {
    return this._source.substring(this._tokenOffset, this._offset);
  }

  // source returns the input
  source(): string {
    return this._source;
  }

  // Value returns the value associated with the last returned token.
  value() {
    return this._value;
  }

  // Copy forks the lexer in its current state.
  copy(): Lexer {
    // Create a new instance of Lexer with the same source
    const copy = new Lexer(this._source);
    
    // Copy all essential state properties
    copy._ch = this._ch;
    copy._offset = this._offset;
    copy._tokenOffset = this._tokenOffset;
    copy._line = this._line;
    copy._tokenLine = this._tokenLine;
    copy._scanOffset = this._scanOffset;
    copy._value = this._value;
    
    return copy;
  }
};
